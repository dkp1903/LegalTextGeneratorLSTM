{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201701062_Project_CharRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MTz1HUvaUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNoF26Rs3PU8",
        "colab_type": "code",
        "outputId": "aab7fbd8-2d21-4d4e-fae1-91720c7a7155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws3IJYly3RVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = '/content/drive/My Drive/dataset_1.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K1o00eA3fE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = sorted(set(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04R_d_DP3mys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rRa-2W336NQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjqDr7kH4DT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BU__KOT4H4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oAwq1Rh4LDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EQR9cZxLAt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMiq-zt4LDpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk4lUwRbLFNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PymPpe4yLIUt",
        "colab_type": "code",
        "outputId": "d8a746dd-473a-439f-f2fd-db330b42e4db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 187) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVqa2KqBLL89",
        "colab_type": "code",
        "outputId": "9924e02f-cfd2-47b0-9081-c2e3397d585c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           47872     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 187)           191675    \n",
            "=================================================================\n",
            "Total params: 4,177,851\n",
            "Trainable params: 4,177,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzGMsuJgLRV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLUxNd3LTxp",
        "colab_type": "code",
        "outputId": "8cda111c-6877-4796-ca19-bf45f19ea9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' the proceedings before an arbitral panel at any time by jointly notifying the chair of the arbitral'\n",
            "\n",
            "Next Char Predictions: \n",
            " ';2--NΙhpO$Üä”4`λEίπΓή£σ$\\n三;r[Br−i〇º−Ö3WΛ>zévwXñb:ΛtÉ£8`u/Εëh’hΝ$œrO[7D«\\nη;ó“Rü倍y≥υκά6e三όºÜ.4°NdYόωwF'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKdj4qnLY8e",
        "colab_type": "code",
        "outputId": "2ad22c10-dff4-4ada-ea9b-12613921e28a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 187)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       5.231803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ10bfuZQnSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFEc619pQpcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqZ-GiwQrwc",
        "colab_type": "code",
        "outputId": "9a412a80-9124-437c-fbea-53e746efe062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1126/1126 [==============================] - 58s 51ms/step - loss: 1.3813\n",
            "Epoch 2/10\n",
            "1126/1126 [==============================] - 58s 51ms/step - loss: 0.8594\n",
            "Epoch 3/10\n",
            "1126/1126 [==============================] - 58s 51ms/step - loss: 0.7737\n",
            "Epoch 4/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.7279\n",
            "Epoch 5/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.7005\n",
            "Epoch 6/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.6842\n",
            "Epoch 7/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.6755\n",
            "Epoch 8/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.6727\n",
            "Epoch 9/10\n",
            "1126/1126 [==============================] - 57s 51ms/step - loss: 0.6736\n",
            "Epoch 10/10\n",
            "1126/1126 [==============================] - 57s 50ms/step - loss: 0.6792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzPbWzYVQvxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a45762d2-6767-4e78-c29f-c5883d94ea7f"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phsVeTgqQyTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtfO1iD0Qz3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6ba373b8-c6e3-4313-8ac9-5d8394842b32"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            47872     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 187)            191675    \n",
            "=================================================================\n",
            "Total params: 4,177,851\n",
            "Trainable params: 4,177,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0FYQxe6Q1QP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDTXL2XOQ4SI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "683db306-70c1-4b78-cdea-6275f1e602e5"
      },
      "source": [
        "print(generate_text(model, start_string=u\"The government: \"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The government: \n",
            "1 \n",
            "\n",
            "Forespect with a view to ensure that the provisions of this Articles shall reduce the importance of representatives other than as wolly in the avoidance of goods infrastructure will following the date of entry into force of this Agreement the Joint Commity to promote the scope of marks granted in relation to intellectual proper e \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "National Treatment), 10.2 (Transfer referred to in this Chapter as would prejudice the legal system. When such measures by a Party from applying measures relating to the entity of a written non-carration of any restrictions specified in determining the qualifying value content of the governmental authority of the exporting Party were required by a situation of the use of a service;   (g) datimume or electronic; and \n",
            "\n",
            "\n",
            "(b) \tair application shall be made unless the Parties may agree. \n",
            "\n",
            "7. A Party shall not simpley as offered ect to governmental authority of the importing Party may standard, and seef- ensure that its competent authorities under preferen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8qFHPWHQ8Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEB3OgmQ--6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buBqUn4IRBbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWPBkIwfREYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08419082-8761-40a4-f968-27424a1fcecf"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {} PPL {}'\n",
        "      print(template.format(epoch+1, batch_n, loss, tf.exp(loss)))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f} PPL : {:.6f}'.format(epoch+1, loss, tf.exp(loss)))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.713625967502594 PPL 2.041379928588867\n",
            "Epoch 1 Batch 100 Loss 0.7431025505065918 PPL 2.1024484634399414\n",
            "Epoch 1 Batch 200 Loss 0.8856378197669983 PPL 2.424530267715454\n",
            "Epoch 1 Batch 300 Loss 0.8207994103431702 PPL 2.272315740585327\n",
            "Epoch 1 Batch 400 Loss 0.7856702208518982 PPL 2.1938767433166504\n",
            "Epoch 1 Batch 500 Loss 0.7744053602218628 PPL 2.169301748275757\n",
            "Epoch 1 Batch 600 Loss 0.7712295651435852 PPL 2.1624233722686768\n",
            "Epoch 1 Batch 700 Loss 0.7761399745941162 PPL 2.173068046569824\n",
            "Epoch 1 Batch 800 Loss 0.6973795890808105 PPL 2.0084826946258545\n",
            "Epoch 1 Batch 900 Loss 0.6993960738182068 PPL 2.0125370025634766\n",
            "Epoch 1 Batch 1000 Loss 0.6461818814277649 PPL 1.9082410335540771\n",
            "Epoch 1 Batch 1100 Loss 0.6841602921485901 PPL 1.9821066856384277\n",
            "Epoch 1 Loss 0.6485 PPL : 1.912649\n",
            "Time taken for 1 epoch 55.80367136001587 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6778233051300049 PPL 1.96958589553833\n",
            "Epoch 2 Batch 100 Loss 0.7087143063545227 PPL 2.0313777923583984\n",
            "Epoch 2 Batch 200 Loss 0.8107252717018127 PPL 2.2495388984680176\n",
            "Epoch 2 Batch 300 Loss 0.7499770522117615 PPL 2.1169514656066895\n",
            "Epoch 2 Batch 400 Loss 0.7739309072494507 PPL 2.1682729721069336\n",
            "Epoch 2 Batch 500 Loss 0.714076817035675 PPL 2.0423004627227783\n",
            "Epoch 2 Batch 600 Loss 0.6677793860435486 PPL 1.9499024152755737\n",
            "Epoch 2 Batch 700 Loss 0.7249082326889038 PPL 2.0645415782928467\n",
            "Epoch 2 Batch 800 Loss 0.6621623039245605 PPL 1.9389803409576416\n",
            "Epoch 2 Batch 900 Loss 0.7151426672935486 PPL 2.044478416442871\n",
            "Epoch 2 Batch 1000 Loss 0.6706838011741638 PPL 1.9555740356445312\n",
            "Epoch 2 Batch 1100 Loss 0.6660041809082031 PPL 1.9464441537857056\n",
            "Epoch 2 Loss 0.6748 PPL : 1.963552\n",
            "Time taken for 1 epoch 55.87218236923218 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6946632266044617 PPL 2.0030343532562256\n",
            "Epoch 3 Batch 100 Loss 0.7239759564399719 PPL 2.062617778778076\n",
            "Epoch 3 Batch 200 Loss 0.7470102906227112 PPL 2.110680341720581\n",
            "Epoch 3 Batch 300 Loss 0.7958956360816956 PPL 2.2164251804351807\n",
            "Epoch 3 Batch 400 Loss 0.6996152997016907 PPL 2.0129783153533936\n",
            "Epoch 3 Batch 500 Loss 0.7085425853729248 PPL 2.031028985977173\n",
            "Epoch 3 Batch 600 Loss 0.6031977534294128 PPL 1.8279547691345215\n",
            "Epoch 3 Batch 700 Loss 0.6393676996231079 PPL 1.8952820301055908\n",
            "Epoch 3 Batch 800 Loss 0.6923763155937195 PPL 1.998458743095398\n",
            "Epoch 3 Batch 900 Loss 0.6763514876365662 PPL 1.966689109802246\n",
            "Epoch 3 Batch 1000 Loss 0.689435601234436 PPL 1.9925905466079712\n",
            "Epoch 3 Batch 1100 Loss 0.639878511428833 PPL 1.8962503671646118\n",
            "Epoch 3 Loss 0.6455 PPL : 1.906851\n",
            "Time taken for 1 epoch 55.98742485046387 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.660962700843811 PPL 1.9366559982299805\n",
            "Epoch 4 Batch 100 Loss 0.6224585175514221 PPL 1.8635038137435913\n",
            "Epoch 4 Batch 200 Loss 0.716678261756897 PPL 2.0476200580596924\n",
            "Epoch 4 Batch 300 Loss 0.7555388808250427 PPL 2.128758430480957\n",
            "Epoch 4 Batch 400 Loss 0.6823703050613403 PPL 1.9785619974136353\n",
            "Epoch 4 Batch 500 Loss 0.7495054602622986 PPL 2.1159534454345703\n",
            "Epoch 4 Batch 600 Loss 0.6507369875907898 PPL 1.9169530868530273\n",
            "Epoch 4 Batch 700 Loss 0.6462187767028809 PPL 1.9083114862442017\n",
            "Epoch 4 Batch 800 Loss 0.6540551781654358 PPL 1.923324465751648\n",
            "Epoch 4 Batch 900 Loss 0.6375171542167664 PPL 1.8917781114578247\n",
            "Epoch 4 Batch 1000 Loss 0.6302158236503601 PPL 1.8780158758163452\n",
            "Epoch 4 Batch 1100 Loss 0.6770980954170227 PPL 1.9681578874588013\n",
            "Epoch 4 Loss 0.6182 PPL : 1.855533\n",
            "Time taken for 1 epoch 55.952473878860474 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6189262866973877 PPL 1.8569331169128418\n",
            "Epoch 5 Batch 100 Loss 0.7052010297775269 PPL 2.0242536067962646\n",
            "Epoch 5 Batch 200 Loss 0.746489405632019 PPL 2.109581232070923\n",
            "Epoch 5 Batch 300 Loss 0.7042118310928345 PPL 2.022252082824707\n",
            "Epoch 5 Batch 400 Loss 0.7467183470726013 PPL 2.1100642681121826\n",
            "Epoch 5 Batch 500 Loss 0.6256754398345947 PPL 1.8695082664489746\n",
            "Epoch 5 Batch 600 Loss 0.6421304941177368 PPL 1.9005255699157715\n",
            "Epoch 5 Batch 700 Loss 0.6554815769195557 PPL 1.9260698556900024\n",
            "Epoch 5 Batch 800 Loss 0.655354917049408 PPL 1.9258259534835815\n",
            "Epoch 5 Batch 900 Loss 0.6587908267974854 PPL 1.932454228401184\n",
            "Epoch 5 Batch 1000 Loss 0.6838768124580383 PPL 1.9815449714660645\n",
            "Epoch 5 Batch 1100 Loss 0.6281430125236511 PPL 1.8741270303726196\n",
            "Epoch 5 Loss 0.6162 PPL : 1.851927\n",
            "Time taken for 1 epoch 55.85190486907959 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6262764930725098 PPL 1.870632290840149\n",
            "Epoch 6 Batch 100 Loss 0.6634594798088074 PPL 1.9414972066879272\n",
            "Epoch 6 Batch 200 Loss 0.6940759420394897 PPL 2.0018584728240967\n",
            "Epoch 6 Batch 300 Loss 0.7892138957977295 PPL 2.201664924621582\n",
            "Epoch 6 Batch 400 Loss 0.6762356758117676 PPL 1.9664613008499146\n",
            "Epoch 6 Batch 500 Loss 0.646241307258606 PPL 1.90835440158844\n",
            "Epoch 6 Batch 600 Loss 0.6079431772232056 PPL 1.836649775505066\n",
            "Epoch 6 Batch 700 Loss 0.5883010625839233 PPL 1.8009262084960938\n",
            "Epoch 6 Batch 800 Loss 0.6405580043792725 PPL 1.8975393772125244\n",
            "Epoch 6 Batch 900 Loss 0.648876965045929 PPL 1.913390874862671\n",
            "Epoch 6 Batch 1000 Loss 0.6370712518692017 PPL 1.8909345865249634\n",
            "Epoch 6 Batch 1100 Loss 0.6185134649276733 PPL 1.8561667203903198\n",
            "Epoch 6 Loss 0.6626 PPL : 1.939801\n",
            "Time taken for 1 epoch 55.998815298080444 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6127409338951111 PPL 1.8454828262329102\n",
            "Epoch 7 Batch 100 Loss 0.6794239282608032 PPL 1.972740888595581\n",
            "Epoch 7 Batch 200 Loss 0.7075892686843872 PPL 2.0290937423706055\n",
            "Epoch 7 Batch 300 Loss 0.8181422352790833 PPL 2.2662856578826904\n",
            "Epoch 7 Batch 400 Loss 0.7176889181137085 PPL 2.0496907234191895\n",
            "Epoch 7 Batch 500 Loss 0.6695742607116699 PPL 1.9534053802490234\n",
            "Epoch 7 Batch 600 Loss 0.623188316822052 PPL 1.8648643493652344\n",
            "Epoch 7 Batch 700 Loss 0.6429355144500732 PPL 1.9020562171936035\n",
            "Epoch 7 Batch 800 Loss 0.643034815788269 PPL 1.902245044708252\n",
            "Epoch 7 Batch 900 Loss 0.6397000551223755 PPL 1.8959121704101562\n",
            "Epoch 7 Batch 1000 Loss 0.6242192983627319 PPL 1.8667879104614258\n",
            "Epoch 7 Batch 1100 Loss 0.6642745733261108 PPL 1.9430803060531616\n",
            "Epoch 7 Loss 0.6077 PPL : 1.836264\n",
            "Time taken for 1 epoch 55.870240688323975 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6295859813690186 PPL 1.8768333196640015\n",
            "Epoch 8 Batch 100 Loss 0.699385404586792 PPL 2.0125155448913574\n",
            "Epoch 8 Batch 200 Loss 0.7259681820869446 PPL 2.0667309761047363\n",
            "Epoch 8 Batch 300 Loss 0.8239467144012451 PPL 2.2794785499572754\n",
            "Epoch 8 Batch 400 Loss 0.7073144316673279 PPL 2.028536081314087\n",
            "Epoch 8 Batch 500 Loss 0.6271504759788513 PPL 1.8722678422927856\n",
            "Epoch 8 Batch 600 Loss 0.6685544848442078 PPL 1.9514144659042358\n",
            "Epoch 8 Batch 700 Loss 0.6967027187347412 PPL 2.0071237087249756\n",
            "Epoch 8 Batch 800 Loss 0.6289864182472229 PPL 1.8757084608078003\n",
            "Epoch 8 Batch 900 Loss 0.6211774945259094 PPL 1.861118197441101\n",
            "Epoch 8 Batch 1000 Loss 0.6463097333908081 PPL 1.908484935760498\n",
            "Epoch 8 Batch 1100 Loss 0.6418802738189697 PPL 1.9000500440597534\n",
            "Epoch 8 Loss 0.6557 PPL : 1.926458\n",
            "Time taken for 1 epoch 55.8504273891449 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6383224725723267 PPL 1.893302083015442\n",
            "Epoch 9 Batch 100 Loss 0.6429380774497986 PPL 1.902061104774475\n",
            "Epoch 9 Batch 200 Loss 0.7445566058158875 PPL 2.1055076122283936\n",
            "Epoch 9 Batch 300 Loss 0.7807521224021912 PPL 2.1831135749816895\n",
            "Epoch 9 Batch 400 Loss 0.7427279949188232 PPL 2.101660966873169\n",
            "Epoch 9 Batch 500 Loss 0.6942358613014221 PPL 2.00217866897583\n",
            "Epoch 9 Batch 600 Loss 0.6121036410331726 PPL 1.8443070650100708\n",
            "Epoch 9 Batch 700 Loss 0.6464844346046448 PPL 1.9088184833526611\n",
            "Epoch 9 Batch 800 Loss 0.6575367450714111 PPL 1.9300323724746704\n",
            "Epoch 9 Batch 900 Loss 0.6610783934593201 PPL 1.9368799924850464\n",
            "Epoch 9 Batch 1000 Loss 0.652818500995636 PPL 1.9209474325180054\n",
            "Epoch 9 Batch 1100 Loss 0.7073045372962952 PPL 2.0285160541534424\n",
            "Epoch 9 Loss 0.7168 PPL : 2.047928\n",
            "Time taken for 1 epoch 55.852948904037476 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6395871639251709 PPL 1.8956979513168335\n",
            "Epoch 10 Batch 100 Loss 0.6626507639884949 PPL 1.9399278163909912\n",
            "Epoch 10 Batch 200 Loss 0.8142485618591309 PPL 2.257478713989258\n",
            "Epoch 10 Batch 300 Loss 0.7763503193855286 PPL 2.173525094985962\n",
            "Epoch 10 Batch 400 Loss 0.7191369533538818 PPL 2.0526609420776367\n",
            "Epoch 10 Batch 500 Loss 0.6963686943054199 PPL 2.006453514099121\n",
            "Epoch 10 Batch 600 Loss 0.6243144273757935 PPL 1.866965651512146\n",
            "Epoch 10 Batch 700 Loss 0.6347813606262207 PPL 1.886609673500061\n",
            "Epoch 10 Batch 800 Loss 0.7375509738922119 PPL 2.090808868408203\n",
            "Epoch 10 Batch 900 Loss 0.6847524046897888 PPL 1.9832806587219238\n",
            "Epoch 10 Batch 1000 Loss 0.7325959205627441 PPL 2.080474376678467\n",
            "Epoch 10 Batch 1100 Loss 0.690585732460022 PPL 1.9948835372924805\n",
            "Epoch 10 Loss 0.6649 PPL : 1.944267\n",
            "Time taken for 1 epoch 55.92764949798584 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}