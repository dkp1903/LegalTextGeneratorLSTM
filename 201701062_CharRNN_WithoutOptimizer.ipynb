{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201701062_CharRNN_WithoutOptimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iokuQUsJI2U",
        "colab_type": "code",
        "outputId": "36423b4f-2478-468f-8d5d-5f3e381fb391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "\"\"\"\n",
        "mounting the google drive to use text data and to clone GItHub repositories\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJHeR_2ESIAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "important libraries imported\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import os, io, sys, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import one_hot\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Activation, Dropout, Input, Lambda, Reshape, Masking\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical, plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-WKyhazY0jD",
        "colab_type": "code",
        "outputId": "aeb77557-963a-4129-c7ab-6346879045fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "making sure tensorflow's version2 is used in this notebook\n",
        "\"\"\"\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import os\n",
        "\n",
        "if int(tf.__version__[0]) < 2:\n",
        "  !pip install tensorflow==2.1\n",
        "\n",
        "print(\"Tensorflow version: \" + tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKm3VjU9Sv0u",
        "colab_type": "code",
        "outputId": "b279c0bc-a7b8-49d0-85f5-e3119338641d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\"\"\"\n",
        "testing if connected to TPU and/or GPU\n",
        "\"\"\"\n",
        "\n",
        "import pprint\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('Not connected to a TPU runtime.')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('Connected to TPU.\\n\\nTPU address is', tpu_address)\n",
        "\n",
        "  with tf.compat.v1.Session((tpu_address)) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "if tf.test.gpu_device_name() == '':\n",
        "  print('\\n\\nNot connected to a GPU runtime.')\n",
        "else:\n",
        "  print('\\n\\nConnected to GPU: ' + tf.test.gpu_device_name())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not connected to a TPU runtime.\n",
            "\n",
            "\n",
            "Connected to GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83PeaGO82CB9",
        "colab_type": "code",
        "outputId": "8eff2c3d-6ffd-4c57-bd8e-eb4a39ecea35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "\"\"\"\n",
        "cleaning the data and forming the examples\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "\n",
        "path = \"/content/drive/My Drive/data_1.txt\"\n",
        "\n",
        "with io.open(path, encoding='utf-8') as corpus:\n",
        "    text = corpus.read()\n",
        "#print(text)\n",
        "\n",
        "LENGTH = len(text)\n",
        "Tx = 11 # length of each example (characters)\n",
        "vocab = sorted(set([word for line in text for word in line.split()]))\n",
        "#vocab = sorted(set(list(text))) # list (a set actually) of all the characters in the corpus\n",
        "print(vocab)\n",
        "char_to_indices = dict((ch, idx) for idx, ch in enumerate(vocab))\n",
        "index_to_char = dict((idx, ch) for idx, ch in enumerate(vocab))\n",
        "\n",
        "# pretty much temporary variables just for the sake of splitting the huge corpus\n",
        "sentences = [] # X\n",
        "mapped_chars = [] # Y\n",
        "\n",
        "step = 3\n",
        "\n",
        "for i in range(0, LENGTH - Tx, step):\n",
        "    temp_text = text[i: i+Tx]\n",
        "    sentences.append(temp_text[:-1])\n",
        "    mapped_chars.append(temp_text[-1])\n",
        "\n",
        "m = len(sentences)\n",
        "print(\"Lenght:\", len(vocab))\n",
        "X = np.zeros((m, Tx - 1, len(vocab)))\n",
        "Y = np.zeros((m, len(vocab)))\n",
        "\n",
        "for i, example in enumerate(sentences):\n",
        "    X[i, :, :] = one_hot([char_to_indices[ch] for ch in example], depth=len(vocab))\n",
        "    Y[i, :] = one_hot(char_to_indices[mapped_chars[i]], depth=len(vocab))\n",
        "\n",
        "# a nuisance is fixed by turning X and Y into numpy arrays\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "#==============printing data dimesions=========================================\n",
        "print(f\"Length of corpus: {LENGTH}\")\n",
        "print(f\"X.shape = {X.shape}\")\n",
        "print(f\"Y.shape = {Y.shape}\")\n",
        "print(f\"Number of examples: {m}\") "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"', '%', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ü', 'í', 'ó', 'ú', 'ü', 'ş', '–', '’', '“', '”', '…', '三', '倍', '安', '晋']\n",
            "Lenght: 88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-05aa0a8bcb83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmapped_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-05aa0a8bcb83>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmapped_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ' '"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kzEifE0OT_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "read the function docstring below\n",
        "\"\"\"\n",
        "\n",
        "def get_example(index=None):\n",
        "    \"\"\"\n",
        "    retrieves the example at index position in X is index is passed, otherwise random example is obtained\n",
        "    :param index: index of example desired to be retrieved\n",
        "    :return: string of text\n",
        "    \"\"\"\n",
        "\n",
        "    if index is None:\n",
        "        index = np.random.randint(low=0, high=m)\n",
        "\n",
        "    curr_x = [index_to_char[idx] for idx in np.argmax(X[index, :, :], axis=1)]\n",
        "    curr_y = index_to_char[np.argmax(Y[index, :])]\n",
        "\n",
        "    x_y = (''.join(curr_x), curr_y)\n",
        "\n",
        "    return x_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpYgE7ygSLTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "testing a single example and the time it took to retrieve it\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.process_time()\n",
        "example = get_example()\n",
        "end = time.process_time()\n",
        "\n",
        "print(f\"Sample X: {example[0]}\\nCorresponding Y: {example[1]}\")\n",
        "print(f\"\\nTime taken for acquiring this example: {end - start} seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPLZQqNSwuah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "network architecture creation\n",
        "model creation\n",
        "plot_model allows me to see what my neural network looks like\n",
        "\"\"\"\n",
        "\n",
        "def Ram_Says(Tx, vocab, output_length):\n",
        "  # network architecture LSTM -> Dropout -> Reshape -> LSTM -> Dropout -> Dense\n",
        "\n",
        "  # define the initial hidden state a0 and initial cell state c0\n",
        "  a0 = Input(shape=(output_length,), name='a0')\n",
        "  c0 = Input(shape=(output_length,), name='c0')\n",
        "  a = a0\n",
        "  c = c0\n",
        "\n",
        "  X = Input(shape=(Tx, len(vocab)), name='X')\n",
        "  \n",
        "  a, _, c = LSTM(units=output_length, activation='tanh', return_state=True, dtype='float32', name=f'lstm_1')(X, [a, c])\n",
        "  a = Dropout(rate=0.2, name=f'dropout_1')(a)\n",
        "  a = Reshape((1, output_length), name='reshape_1')(a) # needed after a dropout layer for another LSTM layer\n",
        "  a = LSTM(units=output_length, activation='tanh', dtype='float32', name=f'lstm_2')(a)\n",
        "  a = Dropout(rate=0.2, name=f'dropout_2')(a)\n",
        "  out = Dense(units=len(vocab), activation='softmax', name=f'dense')(a)\n",
        "    \n",
        "  model = Model(inputs=[X, a0, c0], outputs=out, name='Ram')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GTikSyoerWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "creating the model and the summary of it\n",
        "\"\"\"\n",
        "#====================Creating important variables===============================\n",
        "n_a = 256 # number of hidden state dimensions for each LSTM cell\n",
        "\n",
        "a0 = np.zeros((m, n_a))\n",
        "c0 = np.zeros((m, n_a))\n",
        "#===============================================================================\n",
        "\n",
        "model = Ram_Says(Tx=Tx - 1, vocab=vocab, output_length=n_a)\n",
        "\n",
        "plot_model(model, to_file='/content/drive/My Drive/nn_graph.png')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFzkBWLkMA94",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "configuring optimizations for the model\n",
        "fitting the model\n",
        "\"\"\"\n",
        "\n",
        "learning_rate = 0.01\n",
        "learning_rate_decay = 0.001\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, decay=learning_rate_decay)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 16384\n",
        "\n",
        "model.fit([X, a0, c0], Y, batch_size=batch_size, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXexPfExcpNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "filepath\n",
        "\"\"\"\n",
        "\n",
        "filepath = '/content/drive/My Drive/dkp_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnrScYNaRlFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "this code takes care of saving the new model only if its accuracy is better than\n",
        "that of the last model\n",
        "\"\"\"\n",
        "\n",
        "# if os.path.exists(filepath):\n",
        "#   prev_model = load_model(filepath)\n",
        "#   prev_acc = prev_model.evaluate([X, a0, c0], Y, verbose=0)[1]\n",
        "#   curr_acc = model.evaluate([X, a0, c0], Y, verbose=0)[1]\n",
        "#   if curr_acc > prev_acc:\n",
        "#     print(\"There was a previous model saved.\")\n",
        "#     print(f\"Previous accuracy: {round(prev_acc*100, 2)}%\")\n",
        "#     print(f\"Current accuracy: {round(curr_acc*100, 2)}%\")\n",
        "#     model.save(filepath)\n",
        "#     print('New model is saved.')\n",
        "#   else:\n",
        "#     print(f\"Previous accuracy: {round(prev_acc*100, 2)}%\")\n",
        "#     print(f\"Current accuracy: {round(curr_acc*100, 2)}%\")\n",
        "#     print('Old model is kept.')\n",
        "# else: # if this is the first time saving the model\n",
        "model.save(filepath)\n",
        "print('First time model is saved.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HojRFbuhdgze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "loading the model for sampling\n",
        "\"\"\"\n",
        "\n",
        "Ram_says = load_model(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw-XXtYuYJ-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "testing the accuracy of the model on X and Y\n",
        "\"\"\"\n",
        "\n",
        "accuracy = Ram_says.evaluate([X, a0, c0], Y, verbose=0)[1]\n",
        "print(f\"Accuracy on the training set: {round(accuracy*100, 2)}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHJwSspvMVkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "text sampling time\n",
        "\"\"\"\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds) \n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def generate_output():\n",
        "      diversity = random.choice([0.2, 0.5, 0.7, 1.0, 1.2, 1.4])\n",
        "      diversity = 0.2\n",
        "      print(f'Diversity: {diversity}')\n",
        "      generated = ''\n",
        "      sentence = input('Your text: ')\n",
        "      generated += sentence\n",
        "      if len(sentence) > 10:\n",
        "        sentence = sentence[:10]\n",
        "      elif len(sentence) < 10:\n",
        "        rem = 10 - len(sentence)\n",
        "        sentence += ' ' * rem\n",
        "      sys.stdout.write(generated + ' ')\n",
        "      a0 = np.zeros((1, n_a))\n",
        "      c0 = np.zeros((1, n_a))\n",
        "      sys.stdout.write(sentence)\n",
        "      for i in range(1000):\n",
        "          x_pred = np.zeros((1, Tx-1, len(vocab)))\n",
        "          for t, char in enumerate(sentence):\n",
        "              if char != '0':\n",
        "                  x_pred[0, t, char_to_indices[char]] = 1.\n",
        "          preds = Ram_says.predict([x_pred, a0, c0], verbose=0)[0]\n",
        "          next_index = sample(preds, temperature = 1.0)\n",
        "          next_char = index_to_char[next_index]\n",
        "\n",
        "          generated += next_char\n",
        "          sentence = sentence[1:] + next_char\n",
        "\n",
        "          sys.stdout.write(next_char)\n",
        "          sys.stdout.flush()\n",
        "\n",
        "          if next_char == '\\n':\n",
        "              print('\\n')\n",
        "          elif next_char == '\\t':\n",
        "              print('\\t')\n",
        "\n",
        "generate_output()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}